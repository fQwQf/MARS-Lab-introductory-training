# 读DENSE: Data-Free One-Shot Federated Learning

这篇论文关注一次性联邦学习 (One-Shot Federated Learning, OFL)，着重解决现有OFL方法中的数据依赖和模型同构性假设问题，提出了一种无需数据 (Data-Free) 的方法。

现有的一次性联邦学习方法大多不实用或面临固有限制，例如，需要公共数据集、客户端模型必须是同构的、需要上传额外的数据/模型信息。为了克服这些问题，本文提出了一个新颖的联邦学习框架DENSE，它通过一个数据生成阶段和一个模型蒸馏阶段来训练全局模型。  
DENSE由于以下优点可以在现实中应用：
1. 与其他方法相比，DENSE不需要在客户端和服务器之间传输额外信息（除了模型参数）；
2. DENSE不需要任何辅助数据集进行训练；
3. DENSE考虑了FL中的模型异构性，即不同的客户端可以拥有不同的模型架构。

## 引言

这里首先介绍了一次性联邦学习的优点和有待提高之处。

一次性联邦学习是一个有前景的解决方案，有这样几个好处：
1. 多轮训练在某些场景下不实用，例如模型市场，用户只能从市场购买预训练模型而没有任何真实数据。
2. 频繁的通信带来了很高的被攻击风险。例如，频繁通信很容易被攻击者拦截，他们可以发起中间人攻击，甚至从梯度中重构训练数据。这样，一次性FL由于其单轮特性，可以降低被恶意攻击者拦截的概率。

然而，现有的一次性联邦学习方法均存在局限性：
- 数据集蒸馏会产生额外的通信成本和潜在的隐私泄露风险。
- 基于聚类的方法需要将聚类中心上传到服务器，从而产生额外的通信成本。
- 这些方法都没有考虑模型异构性，即不同客户端拥有不同的模型架构。

对此，本文提出了DENSE框架，它通过一个数据生成阶段和一个模型蒸馏阶段来训练全局模型：在第一阶段，利用集成模型（即客户端上传的本地模型的集成）来训练一个生成器，该生成器可以生成用于第二阶段训练的合成数据。在第二阶段，将集成模型的知识蒸馏到全局模型中。与基于FedAvg的传统FL方法相比，该方法不需要对模型参数进行平均，因此它可以支持异构模型，即客户端可以拥有不同的模型架构。  

## 方法：无需数据的一次性联邦学习 (Data-Free One-Shot Federated Learning)

### 数据生成阶段

第一阶段目标是训练一个生成器来生成合成数据。生成器应该能生成与客户端训练数据具有相似分布的数据。现有的研究通过利用预训练的GAN生成数据，然而，预训练的GAN是在公共数据集上训练的，其数据分布可能与客户端的训练数据不同。此外，需要考虑模型异构性，这使得问题更加复杂。  
为了解决这些问题，本文将训练一个同时考虑相似性、稳定性、和可迁移性的生成器。具体来说，给定一个随机噪声z（从标准高斯分布生成）和一个随机标签y（从均匀分布生成），生成器旨在生成合成数据x = G(z)，使得x与客户端的（带有标签y的）训练数据相似。个人感觉这里和difussion有些类似。  
首先，考虑合成数据x和训练数据之间的相似性。由于无法访问客户端的训练数据，我们不能直接计算合成数据和训练数据之间的相似性。相反，我们首先计算x由集成模型计算出的平均logits（即最后一个全连接层的输出）。
$$D(x; {θ^k}{k=1}^m) = (1/m) Σ{k∈C} f^k(x; θ^k)$$
其中 m = |C|（客户端数量），$D(x; {θ^k})$ 是 x 的平均logits，$θ^k$ 是第k个客户端的参数。而 $f^k(·; θ^k)$ 是客户端k的预测函数，输出给定参数 $θ^k$ 的logits。为简单起见，在本文的其余部分使用 D(x) 表示 $D(x; {θ^k})$。
然后，用以下交叉熵（CE）损失来最小化平均logits和随机标签y：
$$L_CE(x, y; θ_G) = CE(D(x), y)$$
在训练阶段，D(x) 和 y 之间的损失可以轻易降低到几乎为0，这表明合成数据与集成模型完美匹配。  
然而，仅利用CE损失是无法获得高性能的，这可能因为集成模型是在非独立同分布(non-IID)数据上训练的，生成器可能不稳定并陷入局部最有或对合成数据过拟合。  

其次，为了提高生成器的稳定性，本文建议添加一个额外的正则化来稳定训练。
$$L_BN(x; θ_G) = (1/m) Σ_{k∈C} Σ_l (||μ_l(x) - μ_{k,l}||² + ||σ_l²(x) - σ_{k,l}²||) (3)$$
其中 $μ_l(x)$ 和 $σ_l²(x)$ 是对应于生成器$G(·)$的第l个BN层的批处理均值和方差估计，$μ_{k,l}$ 和 $σ_{k,l}²$ 是第k个客户端模型 $f^k(·)$ 的第l个BN层的均值和方差。BN损失最小化了合成数据的特征图统计量与客户端训练数据的特征图统计量之间的距离。因此，无论数据是非独立同分布还是独立同分布，合成数据都可以具有与客户端训练数据相似的分布。  

通过利用CE损失和BN损失，我们可以训练一个能生成合成数据的生成器，但我们观察到合成数据可能远离集成模型的决策边界，这使得集成模型难以将其知识转移给全局模型。  
为了解决这个问题，本文主张生成更多落在集成模型和全局模型决策边界之间的合成数据。位于决策边界同一侧的合成数据，对学习全局模型帮助较小。位于决策边界之间的合成数据，全局模型和集成模型对这些数据有不同的预测，可以帮助全局模型更好地学习集成模型的决策边界。  

受上述观察的启发，本文引入了一个新的边界支持损失 (boundary support loss)，它促使生成器生成更多位于集成模型和全局模型决策边界之间的合成数据。  
具体而言，将合成数据分为两组：
1. 全局模型和集成模型对第一组数据有相同的预测 
$$(arg max_c D^c(x) = arg max_c f_s^c(x; θ_s))$$
2. 对第二组数据有不同的预测 
$$(arg max_c D^c(x) ≠ arg max_c f_s^c(x; θ_s))$$
其中 $D^c(x)$ 和 $f_s^c(x; θ_s)$ 分别是集成模型和全局模型对应第c个标签的logits。第一组数据位于两个决策边界的同一侧，而第二组数据位于决策边界之间。这里使用Kullback-Leibler（KL）散度损失来最大化全局模型和集成模型在第二组数据上预测的差异：
$$L_div(x; θ_G) = -ω KL(D(x), f_s(x; θ_s))$$
其中 $KL(·, ·)$ 表示KL散度损失，$ω = I(arg max_c D^(c)(x) ≠ arg max_c f_s^c(x; θ_s))$ 对于第一组数据输出0，对于第二组数据输出1，I(a)是指示函数，如果a为真则输出1，否则输出0。通过最大化KL散度损失，生成器可以生成更多对模型蒸馏阶段更有帮助的合成数据，并进一步提高集成模型的可迁移性。  
通过结合上述损失，我们可以得到生成器损失如下：
$$L_gen(x, y; θ_G) = L_CE(x, y; θ_G) + λ₁ L_BN(x; θ_G) + λ₂ L_div(x; θ_G)$$ 
其中 λ₁ 和 λ₂ 是缩放因子。



