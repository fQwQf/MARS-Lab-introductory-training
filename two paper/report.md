先来看Does one-shot give the best shot。  
这篇论文由来自匿名单位的匿名作者撰写，应该还没有正式发表。  

所以我们先来了解一下什么是One-shot Federated Learning。通过互联网和LLM，我找到了这个比较好理解的诠释：

标准联邦学习 (FL):
- 多轮迭代: 服务器和客户端之间需要进行很多轮的通信。
- 过程:
    - 服务器将当前的全局模型发送给选定的客户端。
    - 客户端在本地用自己的数据训练这个模型（通常只训练少量几步）。
    - 客户端将模型的更新信息（例如梯度或模型权重变化）发送回服务器。
    - 服务器聚合所有客户端的更新，形成一个新的、改进的全局模型。
    - 重复以上步骤很多次，直到模型收敛。
- 优点: 可以逐步学习，模型精度通常较高。
- 缺点: 通信开销巨大，因为需要反复传输模型或更新。

一次性联邦学习 (OFL):
- 一轮通信: 服务器和客户端之间只有一次主要的通信交互（客户端上传模型）。
- 过程:
    - 客户端（可能从一个初始模型开始，也可能独立从头开始）在本地独立地、长时间地训练自己的模型，直到模型在本地数据上表现良好或者达到预设的训练目标。
    - 训练完成后，每个客户端将自己最终训练好的本地模型一次性上传给服务器。
    - 服务器收集所有客户端上传的最终本地模型。
    - 服务器使用某种聚合策略将这些本地模型一次性合并成一个最终的全局模型。
    - 之后不再有客户端和服务器之间的模型训练相关的通信。
- 优点:
    - 显著降低通信成本: 这是OFL最主要的动机和优势。因为它避免了多轮迭代的巨大通信开销。
    - 简化流程: 协调一次通信比协调多轮要简单。
    - 适用于特定场景: 比如客户端只愿意或只能参与一次模型贡献的场景。
- 缺点:
    - 模型不一致性: 由于客户端长时间独立训练，且它们的数据可能非常不同，导致最终上传的本地模型之间差异巨大，甚至相互冲突。
    - 聚合困难: 简单地平均这些差异巨大的模型，往往得不到好的全局模型。
    - 精度挑战: 传统上，OFL方法的精度通常低于标准的多轮FL，因为聚合步骤面临巨大挑战。

我们回到论文。文中指出现有OFL陷入了一个garbage (inconsistent one-shot local models) in and garbage (degraded global model) out的陷阱，因为：  

1. 模型内部不一致性 (Intra-model inconsistency)
    一个一次性局部模型被证明对于具有相同语义的样本会给出不同的预测。
2. 模型间不一致性 (Inter-model inconsistency)。
    来自不同客户端的不同一次性局部模型表现出不同的参数，导致即使对于相同的样本也会产生不同的预测。

说实话我觉得垃圾进垃圾出在这里被滥用了……这句话应该是描述低价值数据的，而非不一致的模型。  

接下来通过数理演算证明了：  

1. 一次性局部模型在原始样本 (x, y) 和增强样本 (A(x), y) 上的模型内部不一致性可以表示为：
    $$||Δ_intra||² ≥ || (p ⋅ ∇g_a ⋅ ∇A)^T (x - A(x)) ||² > 0$$
    其中 $p = Σ_{c=1}^C (z_c - y_c)$，z 是模型 $w_i$ 对增强样本 A(x) 使用softmax函数激活后的预测，∇g_a 是局部模型 $w_i$ 的梯度，∇A 是数据增强函数的梯度。


    也就是说一次性局部模型中的模型内部不一致性是不可避免的。具体来说，它主要由三个因素引起：(1) 在增强样本上的性能，由 p∇g_a 表示。(2) 数据增强函数的变换属性，由 ∇A 表示。(3) 原始样本和增强样本之间的差异，由 (x - A(x)) 表示。所有这些因素都大于零，导致了模型内部不一致性的存在。

2. 对于任意两个客户端 u 和 v，如果它们拥有相同数量的样本 $n_u = n_v$，那么它们之间模型的一步偏差 $Δinter = ∇W_u - ∇W_v$ 可以表示为：

    $$ ||Δinter||² = || (η/N_u) [ (n_{u,c}(1 - Z_{u,c})X_{u,c} - n_{v,c}(1 - Z_{v,c})X_{v,c}) - (Σ_{c'∈[C_u]} n_{u,c'} Z_{u,c'} X_{u,c'} - Σ_{c'∈[C_v]} n_{v,c'} Z_{v,c'} X_{v,c'}) ] ||² > 0 $$
    其中 η 是学习率，$n_{u,c}$ 和 $n_{v,c}$ 是第 c 类别的样本数量，c' 是除了 c 之外的负类。

    那么是说对于任何两个客户端，每个局部训练步骤都会导致一次性局部模型的不一致性。随着局部训练步骤的增加，模型间的不一致性变得更加显著

现有的OFL方法侧重于设计服务器端的聚合机制：
1. 基于优化的方法 (Optimization-based methods) 
2. 基于蒸馏的方法 (Distillation-based methods) 
3. 生成方法 (Generative methods) 
4. 基于选择性集成的方法 (Selective ensemble-based methods) 

然而，当前的OFL局部训练策略不可避免地导致“垃圾”一次性局部模型，这给服务器端的聚合带来了挑战。

因此，论文提出了一个新的OFL框架，名为FAFI。它包含两个组件：自对齐局部学习(Self-Alignment Local Learning)和信息感知特征融合推理 (Informative Feature Fused Inference)：

- 在客户端，训练特征提取器，使其能够学习到可以泛化到不同增强样本的不变特征。作者还设计了类别原型学习以获得独特的原型，替换了原始的分类器，从而减轻了预测的负面影响。
- 在服务器端，将所有客户端的原型聚合成一个全局原型。在推理阶段，框架信息感知地融合由局部模型提取的特征，以缓解模型间的不一致性。

***自对齐局部学习 (Self-Alignment Local Learning)***

注意到导致模型内部不一致性的关键因素是模型无法处理具有相同语义的增强样本，即当 $∇g_a > 0$ 时。然而，当前的监督学习范式只能基于标签和原始输入学习固定的语义，缺乏对多样化增强样本的泛化能力。如果模型在任何增强样本上都能表现良好，即 $∇g_a = 0$，那么模型内部不一致性将得到缓解。似乎直接采用数据增强可以解决这个问题，然而，受监督训练范式的约束，只要使用标签进行训练 (p ≠ 0)，模型内部不一致性就将不可避免地发生。为此，作者引入自对齐学习来学习不变特征和一个能够泛化到多样化增强样本的无偏分类器。

***信息感知特征融合推理 (Informative Feature Fused Inference)***:

前面证明了数据异构性引起的模型间不一致性是不可避免的。由于局部模型之间存在显著差异，通过参数级别的聚合难以重构一个表现良好的全局模型，这是导致性能低下的根本原因。那么很容易想到，可以不聚合模型参数，而是融合由不一致的局部模型提取的特征来整合语义，从而减轻数据异构性的负面影响并增强推理能力。此外，注意到不同模型提取的特征在融合过程中表现出差异，为了解决这个问题，作者设计了一种基于注意力的特征融合机制。